{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muskanrath30/muskanrath30/blob/main/WordEmbeddingPlayground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Exercise: Letâ€™s play with Word Embeddings!\n",
        "\n",
        "A Word Embedding is a representation of a word. Embeddings are vectors of some fixed number of values, where each value is typically a float between 0 and 1. We use `representation`, `embedding` or `vector` pretty much interchangeably. They all mean the same thing: a list of numbers that are mean to represent a word. \n",
        "\n",
        "In this class exercise, we will use common libraries to load, view and interact with some word embeddings. \n",
        "\n",
        "\n",
        "Thanks to [here](https://colab.research.google.com/github/ecs-vlc/COMP6248/blob/master/docs/labs/lab7/7_2_WordEmbeddings.ipynb#scrollTo=vWwACkpZx2Eu) for inspiration."
      ],
      "metadata": {
        "id": "NGyZS6JuvgXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load GloVe vectors\n",
        "First, we will load some word embeddings that have already been trained. The original GloVe paper introduced vectors of different sizes (no. of float values used in each representation) for each word. We will use the one of size 100. This size is also referred to as the `dimensions`. \n",
        "\n"
      ],
      "metadata": {
        "id": "2g9dydMDxow_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext.vocab, torch\n",
        "\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
        "\n",
        "print(f'There are {len(glove.itos)} words in the vocabulary')"
      ],
      "metadata": {
        "id": "lUZeuqhoxtR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each unique word is assigned an `index`. For example, the word `the` is assigned an index of 0. "
      ],
      "metadata": {
        "id": "LUrt2G4-zsBC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q35MqVmJ-Dap"
      },
      "outputs": [],
      "source": [
        "# Fetch the integer index associated with the word.\n",
        "glove.stoi['the']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the word associated with an integer index.\n",
        "glove.itos[0]"
      ],
      "metadata": {
        "id": "yFz7uzuD5nsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings are oraganized as a 2D matrix. "
      ],
      "metadata": {
        "id": "L_cSUT7Dz350"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove.vectors.shape"
      ],
      "metadata": {
        "id": "KCdwFEs80DUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you pay close attention, you will recognize that the second item in the output above, `100` is the number of dimensions used to represent each word. \n",
        "The first element is the count of number of unique words for which we have embeddings. This is also referred to as the `vocabulary`."
      ],
      "metadata": {
        "id": "4yEXvOiK0F0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods to be implemented\n"
      ],
      "metadata": {
        "id": "3fcSzR5Z0h0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(glove, word):\n",
        "  # First, identify the index associated with the chosen word.\n",
        "  # Then, fetch the corresponding embedding from glove.vectors \n",
        "  return glove.vectors[glove.stoi[word]]"
      ],
      "metadata": {
        "id": "0MB8FcQZ0hYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_neighbor(glove, query_embedding, n=10):\n",
        "  # Find the nearest neighbors for a given embedding. Here's the information you will need.\n",
        "  # torch.dist(v1, v2): this method returns the distance between two vectors.\n",
        "\n",
        "  # First, iterate through all words using the itos method introduced above.\n",
        "  # Compute the distance between each word in the vocabulary and the query_embedding\n",
        "  # Sort the words by their distance wrt query_embedding. Return n nearest neighbors.\n",
        "  distances = [(w, torch.dist(query_embedding, get_embedding(glove, w)).item()) for w in glove.itos]\n",
        "  return sorted(distances, key = lambda w: w[1])[:n]\n",
        "  "
      ],
      "metadata": {
        "id": "BasWMhZb0b0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbor(glove, get_embedding(glove, \"india\"))"
      ],
      "metadata": {
        "id": "eOlPMfiiDgn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the above methods are implemented, play around with it. Specifically, get the nearest neighbors for the following words:\n",
        "\n",
        "- bank\n",
        "- seattle\n",
        "- jaguar\n",
        "- neural\n",
        "- brazil\n",
        "- france\n",
        "- india\n",
        "- [other words of your choice]"
      ],
      "metadata": {
        "id": "T7w7o6tO2Yxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbor(glove, get_embedding(glove, \"bank\"))"
      ],
      "metadata": {
        "id": "x4MkBvvR7Jr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbor(glove, get_embedding(glove, \"seattle\"))"
      ],
      "metadata": {
        "id": "U7tJpYUM7NKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbor(glove, get_embedding(glove, \"jaguar\"))"
      ],
      "metadata": {
        "id": "2XyR2gkL7RCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbor(glove, get_embedding(glove, \"neural\"))"
      ],
      "metadata": {
        "id": "xKvpsCvB7VAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbor(glove, get_embedding(glove, \"brazil\"))"
      ],
      "metadata": {
        "id": "8OBX8Cpa7c9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbor(glove, get_embedding(glove, \"malayalam\"))"
      ],
      "metadata": {
        "id": "m0_tqj_A7eiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbor(glove, get_embedding(glove, \"odia\"))"
      ],
      "metadata": {
        "id": "VsNM2k277ltA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbor(glove, get_embedding(glove, \"maharashtra\"))"
      ],
      "metadata": {
        "id": "CZau_j7H7ryO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbor(glove, get_embedding(glove, \"tamil\"))"
      ],
      "metadata": {
        "id": "wCxqlcBT7jgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbor(glove, get_embedding(glove, \"france\"))"
      ],
      "metadata": {
        "id": "pKePQXSL7501"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbor(glove, get_embedding(glove, \"apple\"))"
      ],
      "metadata": {
        "id": "aj0g_-li8Blw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9yPWAJC58HOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above examples, we may face issues when it comes to Polysemy words. Polysemy words are those which have multiple related meanings."
      ],
      "metadata": {
        "id": "7d8_40Do8H0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analogies\n",
        "\n",
        "One of the first extremely surprising findings of word embeddings was their ability to solve simple analogies. Let's first implement the method.\n"
      ],
      "metadata": {
        "id": "C9NhIaEOvewz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D0vHTLO4CLNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analogy(glove, word1, word2, word3, n=5):\n",
        "  candidate_words = nearest_neighbor(glove, get_embedding(glove, word2) - get_embedding(glove, word1) + get_embedding(glove, word3), n=5)\n",
        "  filtered_candidate_words = [x for x in candidate_words if x[0] not in [word1, word2, word3]]\n",
        "  return filtered_candidate_words[:n]\n"
      ],
      "metadata": {
        "id": "iaCh17T26vQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogy(glove, \"france\", \"paris\", \"india\")  # this means france: paris = india: <which word?>\n",
        "# Here we are using nearest neighbor method as well"
      ],
      "metadata": {
        "id": "L2dhVZ05GJQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try the above method to solve the following analogical problems:\n",
        "\n",
        "1. man:king :: woman : ___ ?\n",
        "2. france:paris :: brazil: ___ ?\n",
        "3. good:best :: funny: ___ ?\n",
        "4. baseball:diamond :: tennis: ___ ?\n"
      ],
      "metadata": {
        "id": "sjofTjC86bjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analogy(glove, \"man\", \"king\", \"woman\")"
      ],
      "metadata": {
        "id": "bDWd0Lk18V6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogy(glove, \"france\", \"paris\", \"brazil\")"
      ],
      "metadata": {
        "id": "EBEVHPqj8XsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogy(glove, \"good\", \"best\", \"funny\")"
      ],
      "metadata": {
        "id": "B0-6X4RZ8gxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogy(glove, \"baseball\", \"diamond\", \"tennis\")"
      ],
      "metadata": {
        "id": "TpAJI-Nv9VFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogy(glove, \"india\", \"maharashtra\", \"united states of america\")"
      ],
      "metadata": {
        "id": "-xT9mrv0-fLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogy(glove, \"india\", \"maharashtra\", \"usa\")"
      ],
      "metadata": {
        "id": "m5-D15BY-har"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observed that the words are case sensitive. Capitalised words are entities, the people who developed this model included only lower case words in the word corpus.                          \n",
        "Each entity is one token. Multiple tokens weren't considered\n",
        "\n",
        "united states of america is same as usa but the same names do not have similar embeddings"
      ],
      "metadata": {
        "id": "AHg9wH1n9kPm"
      }
    }
  ]
}